{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "771dae7d-366a-40f2-9120-22ffa252fbf6",
   "metadata": {},
   "source": [
    "# PyTorch Resnet50 Benchmarking Sample.\n",
    "\n",
    "This sample code will run a simple inference loop to estimate the inference time for running Resnet50 on PyTorch. \n",
    "\n",
    "To compare the results, we added the native and also OpenVINO inference for quick time comparison with IPEX.\n",
    "\n",
    "Note:\n",
    "Please check for BF16 supports.\n",
    "https://www.intel.com/content/www/us/en/products/docs/processors/xeon-accelerated/4th-gen-xeon-scalable-processors.html\n",
    "\n",
    "DevCloud:\n",
    "https://www.intel.com/content/www/us/en/developer/tools/devcloud/services.html\n",
    "\n",
    "@author: Raymond Lo, PhD (AI Software Evangelist Global Lead)\n",
    "\n",
    "@disclaimer: This is an initial internal test and all benchmark results below need to be validated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6a625a7-7640-49b9-ba9e-6cd46778491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from intel_extension_for_pytorch.quantization import prepare, convert\n",
    "import intel_extension_for_pytorch as ipex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6383a17b-d5c4-428c-bf23-18bfcaef9187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, data, iterations):\n",
    "  with torch.no_grad():\n",
    "    # warm up\n",
    "    for _ in range(100):\n",
    "      model(data)\n",
    "\n",
    "    # measure\n",
    "    import time\n",
    "    start = time.time()\n",
    "    for _ in range(iterations):\n",
    "      output = model(data)\n",
    "    end = time.time()\n",
    "    print('Inference took {:.2f} ms on average'.format((end-start)/iterations*1000))\n",
    "    print('Total time: {:.2f} s'.format(end-start)+\" for \"+str(iterations)+\" runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "febbbf61-2162-4111-9942-7caddfb3bd7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
    "model.eval()\n",
    "\n",
    "dtype = 'float32'\n",
    "torchscript = 'False'\n",
    "\n",
    "data = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "model = model.to(memory_format=torch.channels_last)\n",
    "data = data.to(memory_format=torch.channels_last)\n",
    "\n",
    "if dtype == 'float32':\n",
    "    model = ipex.optimize(model, dtype=torch.float32)\n",
    "elif dtype == 'bfloat16':\n",
    "    model = ipex.optimize(model, dtype=torch.bfloat16)\n",
    "else: # int8\n",
    "    from intel_extension_for_pytorch.quantization import prepare, convert\n",
    "    qconfig = ipex.quantization.default_static_qconfig\n",
    "    model = prepare(model, qconfig, example_inputs=data, inplace=False)\n",
    "    # calibration\n",
    "    n_iter = 100\n",
    "    for i in range(n_iter):\n",
    "        model(data)\n",
    "\n",
    "    model = convert(model)\n",
    "\n",
    "with torch.cpu.amp.autocast(enabled=dtype=='bfloat16'):\n",
    "    if torchscript: \n",
    "        with torch.no_grad():\n",
    "            model = torch.jit.trace(model, data)\n",
    "            model = torch.jit.freeze(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a27ea56-bb5d-4c6b-9752-90964a4e3552",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc18bb10-35d6-4c1b-bfb1-c44295f740b6",
   "metadata": {},
   "source": [
    "## Run the inference with IPEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8077ba09-f59f-4854-87ed-f6be4ffa070a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference took 4.37 ms on average\n",
      "Total time: 4.37 s for 1000 runs\n"
     ]
    }
   ],
   "source": [
    "inference(model, data, n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd5be1a-6db4-44e7-ae46-7c90267a853d",
   "metadata": {},
   "source": [
    "## Run the inference with the native PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cf8b1808-eba0-4dc6-9975-274d9f076f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_native = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
    "model_native.eval()\n",
    "\n",
    "model_native = model_native.to(memory_format=torch.channels_last)\n",
    "data = data.to(memory_format=torch.channels_last)\n",
    "\n",
    "# calibration\n",
    "n_iter = 100\n",
    "for i in range(n_iter):\n",
    "    model_native(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a5cd180-1262-4300-a8a2-9b2715868b6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference took 12.19 ms on average\n",
      "Total time: 12.19 s for 1000 runs\n"
     ]
    }
   ],
   "source": [
    "inference(model_native, data, n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2952e87-d09d-43fa-972c-e6040095294c",
   "metadata": {},
   "source": [
    "## OpenVINO Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "74218a7d-6430-4595-9f39-c4cf400a9dbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openvino.runtime import Core, AsyncInferQueue\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa255ec9-4fdb-4c35-a488-e2dbe7935f71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference_openvino(ov_model, data, output_layer_onnx, iterations):\n",
    "    # warm up\n",
    "    for _ in range(100):\n",
    "        output = ov_model([data])\n",
    "\n",
    "    # measure\n",
    "    import time\n",
    "    start = time.time()\n",
    "    for _ in range(iterations):\n",
    "        output = ov_model([data])\n",
    "        #output = ov_model([data])[output_layer_onnx]\n",
    "        #print(output)\n",
    "        \n",
    "    end = time.time()\n",
    "    print('Inference took {:.2f} ms on average'.format((end-start)/iterations*1000))\n",
    "    print('Total time: {:.2f} s'.format(end-start)+\" for \"+str(iterations)+\" runs\")\n",
    "    \n",
    "#just callback and take data back (no post-processing)\n",
    "def callback(infer_request, info) -> None:\n",
    "    res = infer_request.get_output_tensor(0).data[0]\n",
    "    #print(res)\n",
    "    \n",
    "def inference_openvino_async(ov_model, data, output_layer_onnx, iterations):\n",
    "    import time\n",
    "    #0 will pick the optimal # with auto plugin throughput mode\n",
    "    infer_queue = AsyncInferQueue(ov_model, 0)\n",
    "    infer_queue.set_callback(callback)\n",
    "    \n",
    "    #measure the total time it takes to process all requests.\n",
    "    start = time.time()\n",
    "    for _ in range(iterations):\n",
    "        infer_queue.start_async({'input.1': data})\n",
    "        #infer_queue.start_async({0: data}, output_layer_onnx)\n",
    "\n",
    "    infer_queue.wait_all()\n",
    "    end = time.time()\n",
    "    print('Inference took {:.2f} ms on average'.format((end-start)/iterations*1000))\n",
    "    print('Total time: {:.2f} s'.format(end-start)+\" for \"+str(iterations)+\" runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1ce20b07-0bb8-463b-9059-210b5a0a243a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.0+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Model Optimizer command to convert the ONNX model to OpenVINO:\n",
      "mo --input_model \"resnet50.onnx\" --output_dir \".\" --compress_to_fp16\n",
      "Exporting ONNX model to IR...\n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /home/raymondlo84/pytorch_openvino/intel-extension-for-pytorch/examples/notebooks/resnet50.xml\n",
      "[ SUCCESS ] BIN file: /home/raymondlo84/pytorch_openvino/intel-extension-for-pytorch/examples/notebooks/resnet50.bin\n"
     ]
    }
   ],
   "source": [
    "onnx_path = \"resnet50.onnx\"\n",
    "ir_path = \".\"\n",
    "ir_file = \"resnet50.xml\"\n",
    "\n",
    "torch.onnx.export(\n",
    "            model_native,\n",
    "            data,\n",
    "            onnx_path,\n",
    "        )\n",
    "\n",
    "# Construct the command for Model Optimizer.\n",
    "mo_command = f\"\"\"mo\n",
    "                 --input_model \"{onnx_path}\"\n",
    "                 --output_dir \"{ir_path}\"\n",
    "                 --compress_to_fp16\n",
    "                 \"\"\"\n",
    "mo_command = \" \".join(mo_command.split())\n",
    "print(\"Model Optimizer command to convert the ONNX model to OpenVINO:\")\n",
    "print(mo_command)\n",
    "\n",
    "print(\"Exporting ONNX model to IR...\")\n",
    "mo_result = %sx $mo_command\n",
    "print(\"\\n\".join(mo_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "85d73b17-8b46-4c2f-aaba-458a8709bca0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: Intel(R) Xeon(R) Gold 6348 CPU @ 2.60GHz\n",
      "GPU: Intel(R) Data Center GPU Flex Series 170 [0x56c0] (dGPU)\n"
     ]
    }
   ],
   "source": [
    "# Load the network to OpenVINO Runtime.\n",
    "ie = Core()\n",
    "devices = ie.available_devices\n",
    "ie.set_property({'CACHE_DIR': 'cache'})\n",
    "\n",
    "for device in devices:\n",
    "    device_name = ie.get_property(device, \"FULL_DEVICE_NAME\")\n",
    "    print(f\"{device}: {device_name}\")\n",
    "\n",
    "num_cores = os.cpu_count() \n",
    "\n",
    "model_onnx = ie.read_model(model=onnx_path)\n",
    "\n",
    "#CPU only tests\n",
    "compiled_model_onnx = ie.compile_model(model=model_onnx, device_name=\"CPU\")\n",
    "compiled_model_onnx_throughput = ie.compile_model(model=model_onnx, device_name=\"CPU\", config={\"PERFORMANCE_HINT\":\"THROUGHPUT\"})\n",
    "\n",
    "#GPU tests\n",
    "#compiled_model_onnx_throughput_gpu = ie.compile_model(model=model_onnx, device_name=\"GPU\", config={\"PERFORMANCE_HINT\":\"THROUGHPUT\"})\n",
    "\n",
    "output_layer_onnx = compiled_model_onnx.output(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "65b607c1-fd9b-43a1-ac81-afe33a1fc722",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference took 8.66 ms on average\n",
      "Total time: 8.66 s for 1000 runs\n"
     ]
    }
   ],
   "source": [
    "# Run inference on the input image. (sync mode) with latency priority\n",
    "inference_openvino(compiled_model_onnx, data, output_layer_onnx, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d0c641d0-9da6-4774-b273-abae7e36bc24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference took 1.38 ms on average\n",
      "Total time: 1.38 s for 1000 runs\n"
     ]
    }
   ],
   "source": [
    "# Run inference on the input image. (async mode) \n",
    "#print(output_layer_onnx)\n",
    "#input_layer_onnx = compiled_model_onnx_throughput.input(0)\n",
    "#print(input_layer_onnx)\n",
    "inference_openvino_async(compiled_model_onnx_throughput, data, output_layer_onnx, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a76017e4-e06a-42c0-9c2f-ae3b42be5e50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference took 1.24 ms on average\n",
      "Total time: 12.41 s for 10000 runs\n"
     ]
    }
   ],
   "source": [
    "#Run inference on GPUs - uncomment to try\n",
    "#inference_openvino_async(compiled_model_onnx_throughput_gpu, data, output_layer_onnx, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d3da3a75-7027-4897-b5bb-e5b96f45230c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# async is needed to get best run performance out from all cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ad392e02-fc6b-484b-8144-871bc8f09baf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[ WARNING ] Performance hint was not explicitly specified in command line. Device(CPU) performance hint will be set to LATENCY.\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 51.75 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input.1 (node: input.1) : f32 / [...] / [1,3,224,224]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     495 (node: 495) : f32 / [...] / [1,1000]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input.1 (node: input.1) : u8 / [N,C,H,W] / [1,3,224,224]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     495 (node: 495) : f32 / [...] / [1,1000]\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 394.59 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: torch_jit\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 2\n",
      "[ INFO ]   NUM_STREAMS: 2\n",
      "[ INFO ]   AFFINITY: Affinity.CORE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 56\n",
      "[ INFO ]   PERF_COUNT: False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT: PerformanceMode.LATENCY\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'input.1'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'input.1' with random values \n",
      "[Step 10/11] Measuring performance (Start inference synchronously, limits: 30000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 38.66 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "[ INFO ] Count:            2506 iterations\n",
      "[ INFO ] Duration:         30014.46 ms\n",
      "[ INFO ] Latency:\n",
      "[ INFO ]    Median:        5.25 ms\n",
      "[ INFO ]    Average:       11.88 ms\n",
      "[ INFO ]    Min:           4.25 ms\n",
      "[ INFO ]    Max:           45.44 ms\n",
      "[ INFO ] Throughput:   190.34 FPS\n"
     ]
    }
   ],
   "source": [
    "!benchmark_app -m resnet50.xml -t 30 -api sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0f30be10-0316-4879-aa8c-7b724fe46f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see the median average and min for all inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f004bfa5-571c-45b0-b7f3-824187ba837a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[ WARNING ] Performance hint was not explicitly specified in command line. Device(CPU) performance hint will be set to THROUGHPUT.\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 60.84 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input.1 (node: input.1) : f32 / [...] / [1,3,224,224]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     495 (node: 495) : f32 / [...] / [1,1000]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input.1 (node: input.1) : u8 / [N,C,H,W] / [1,3,224,224]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     495 (node: 495) : f32 / [...] / [1,1000]\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 3139.80 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: torch_jit\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 56\n",
      "[ INFO ]   NUM_STREAMS: 56\n",
      "[ INFO ]   AFFINITY: Affinity.CORE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 56\n",
      "[ INFO ]   PERF_COUNT: False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT: PerformanceMode.THROUGHPUT\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'input.1'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'input.1' with random values \n",
      "[Step 10/11] Measuring performance (Start inference asynchronously, 56 inference requests, limits: 30000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 90.84 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "[ INFO ] Count:            23800 iterations\n",
      "[ INFO ] Duration:         30083.56 ms\n",
      "[ INFO ] Latency:\n",
      "[ INFO ]    Median:        70.07 ms\n",
      "[ INFO ]    Average:       70.45 ms\n",
      "[ INFO ]    Min:           59.78 ms\n",
      "[ INFO ]    Max:           130.04 ms\n",
      "[ INFO ] Throughput:   791.13 FPS\n"
     ]
    }
   ],
   "source": [
    "!benchmark_app -m resnet50.xml -t 30 -api async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2f8269ad-ee53-4465-a2e9-c330e6397229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see the median average and min for all inferences-- and async usually have a slightly higher latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4d369d40-2a76-4aac-97ac-881226afec28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] GPU\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[ WARNING ] Performance hint was not explicitly specified in command line. Device(GPU) performance hint will be set to THROUGHPUT.\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 32.36 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input.1 (node: input.1) : f32 / [...] / [1,3,224,224]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     495 (node: 495) : f32 / [...] / [1,1000]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input.1 (node: input.1) : u8 / [N,C,H,W] / [1,3,224,224]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     495 (node: 495) : f32 / [...] / [1,1000]\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 13446.41 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 1024\n",
      "[ INFO ]   NETWORK_NAME: torch_jit\n",
      "[ INFO ]   AUTO_BATCH_TIMEOUT: 1000\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'input.1'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'input.1' with random values \n",
      "[Step 10/11] Measuring performance (Start inference asynchronously, 1024 inference requests, limits: 30000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 29.06 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "[ INFO ] Count:            162816 iterations\n",
      "[ INFO ] Duration:         30286.49 ms\n",
      "[ INFO ] Latency:\n",
      "[ INFO ]    Median:        188.80 ms\n",
      "[ INFO ]    Average:       188.33 ms\n",
      "[ INFO ]    Min:           101.58 ms\n",
      "[ INFO ]    Max:           206.81 ms\n",
      "[ INFO ] Throughput:   5375.86 FPS\n"
     ]
    }
   ],
   "source": [
    "#GPU benchmark - uncomment to try\n",
    "#!benchmark_app -m resnet50.xml -t 30 -api async -d GPU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
