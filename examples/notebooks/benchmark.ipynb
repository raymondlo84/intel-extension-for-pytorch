{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "771dae7d-366a-40f2-9120-22ffa252fbf6",
   "metadata": {},
   "source": [
    "# PyTorch Resnet50 Benchmarking Sample.\n",
    "\n",
    "This sample code will run a simple inference loop to estimate the inference time for running Resnet50 on PyTorch. \n",
    "\n",
    "To compare the results, we added the native and also OpenVINO inference for quick time comparison with IPEX.\n",
    "\n",
    "Note:\n",
    "Please check for BF16 supports.\n",
    "https://www.intel.com/content/www/us/en/products/docs/processors/xeon-accelerated/4th-gen-xeon-scalable-processors.html\n",
    "\n",
    "DevCloud:\n",
    "https://www.intel.com/content/www/us/en/developer/tools/devcloud/services.html\n",
    "\n",
    "@author: Raymond Lo, PhD (AI Software Evangelist Global Lead)\n",
    "\n",
    "@disclaimer: This is an initial internal test and all benchmark results below need to be validated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ca76bd-6f84-4e02-a643-2bc94e4b231b",
   "metadata": {},
   "source": [
    "# How to Run (Linux)\n",
    "\n",
    "### Setup Virtual Environment\n",
    "```\n",
    "python3 -m venv ipex_openvino_env\n",
    "source ipex_openvino_env/bin/activate\n",
    "```\n",
    "### Install OpenVINO\n",
    "\n",
    "```\n",
    "pip install --upgrade pip\n",
    "pip install wheel setuptools\n",
    "pip install openvino-dev \n",
    "```\n",
    "\n",
    "### Install IPEX\n",
    "\n",
    "```\n",
    "pip install intel_extension_for_pytorch\n",
    "pip install intel_extension_for_pytorch -f https://developer.intel.com/ipex-whl-stable-cpu\n",
    "\n",
    "```\n",
    "\n",
    "### Install Jupyter Lab\n",
    "\n",
    "```\n",
    "pip install jupyterlab\n",
    "\n",
    "```\n",
    "\n",
    "### Run Jupyter Lab\n",
    "```\n",
    "jupyter lab examples/notebooks \n",
    "```\n",
    "\n",
    "Then, open benchmark.ipynb and run the notebook :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6a625a7-7640-49b9-ba9e-6cd46778491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from intel_extension_for_pytorch.quantization import prepare, convert\n",
    "import intel_extension_for_pytorch as ipex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6383a17b-d5c4-428c-bf23-18bfcaef9187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, data, iterations):\n",
    "  with torch.no_grad():\n",
    "    # warm up\n",
    "    for _ in range(100):\n",
    "      model(data)\n",
    "\n",
    "    # measure\n",
    "    import time\n",
    "    start = time.time()\n",
    "    for _ in range(iterations):\n",
    "      output = model(data)\n",
    "    end = time.time()\n",
    "    print('Inference took {:.2f} ms in average'.format((end-start)/iterations*1000))\n",
    "    print('Total time: {:.2f} s'.format(end-start)+\" for \"+str(iterations)+\" runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "febbbf61-2162-4111-9942-7caddfb3bd7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
    "model.eval()\n",
    "\n",
    "dtype = 'float32'\n",
    "torchscript = 'False'\n",
    "\n",
    "data = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "model = model.to(memory_format=torch.channels_last)\n",
    "data = data.to(memory_format=torch.channels_last)\n",
    "\n",
    "if dtype == 'float32':\n",
    "    model = ipex.optimize(model, dtype=torch.float32)\n",
    "elif dtype == 'bfloat16':\n",
    "    model = ipex.optimize(model, dtype=torch.bfloat16)\n",
    "else: # int8\n",
    "    from intel_extension_for_pytorch.quantization import prepare, convert\n",
    "\n",
    "qconfig = ipex.quantization.default_static_qconfig\n",
    "model = prepare(model, qconfig, example_inputs=data, inplace=False)\n",
    "\n",
    "# calibration\n",
    "n_iter = 100\n",
    "for i in range(n_iter):\n",
    "    model(data)\n",
    "\n",
    "model = convert(model)\n",
    "\n",
    "with torch.cpu.amp.autocast(enabled=dtype=='bfloat16'):\n",
    "    if torchscript: \n",
    "        with torch.no_grad():\n",
    "            model = torch.jit.trace(model, data)\n",
    "            model = torch.jit.freeze(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a27ea56-bb5d-4c6b-9752-90964a4e3552",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc18bb10-35d6-4c1b-bfb1-c44295f740b6",
   "metadata": {},
   "source": [
    "## Run the inference with IPEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8077ba09-f59f-4854-87ed-f6be4ffa070a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference took 4.16 ms in average\n",
      "Total time: 12.47 s for 3000 runs\n"
     ]
    }
   ],
   "source": [
    "inference(model, data, n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd5be1a-6db4-44e7-ae46-7c90267a853d",
   "metadata": {},
   "source": [
    "## Run the inference with the native PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf8b1808-eba0-4dc6-9975-274d9f076f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_native = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
    "model_native.eval()\n",
    "\n",
    "model_native = model_native.to(memory_format=torch.channels_last)\n",
    "data = data.to(memory_format=torch.channels_last)\n",
    "\n",
    "# calibration\n",
    "n_iter = 100\n",
    "for i in range(n_iter):\n",
    "    model_native(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a5cd180-1262-4300-a8a2-9b2715868b6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference took 12.49 ms in average\n",
      "Total time: 37.48 s for 3000 runs\n"
     ]
    }
   ],
   "source": [
    "inference(model_native, data, n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2952e87-d09d-43fa-972c-e6040095294c",
   "metadata": {},
   "source": [
    "## OpenVINO Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74218a7d-6430-4595-9f39-c4cf400a9dbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openvino.runtime import Core, AsyncInferQueue\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa255ec9-4fdb-4c35-a488-e2dbe7935f71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference_openvino(ov_model, data, output_layer_onnx, iterations):\n",
    "    # warm up\n",
    "    for _ in range(100):\n",
    "        ov_model([data])[output_layer_onnx]\n",
    "\n",
    "    # measure\n",
    "    import time\n",
    "    start = time.time()\n",
    "    for _ in range(iterations):\n",
    "        output = ov_model([data])[output_layer_onnx]\n",
    "    end = time.time()\n",
    "    print('Inference took {:.2f} ms in average'.format((end-start)/iterations*1000))\n",
    "    print('Total time: {:.2f} s'.format(end-start)+\" for \"+str(iterations)+\" runs\")\n",
    "    \n",
    "#just callback and take data back (no post-processing)\n",
    "def callback(infer_request, info) -> None:\n",
    "    res = infer_request.get_output_tensor(0).data[0]\n",
    "    \n",
    "def inference_openvino_async(ov_model, data, output_layer_onnx, iterations):\n",
    "    import time\n",
    "    infer_queue = AsyncInferQueue(ov_model, 2)\n",
    "    infer_queue.set_callback(callback)\n",
    "    \n",
    "    #measure the total time it takes to process all requests.\n",
    "    start = time.time()\n",
    "    for _ in range(iterations):\n",
    "        infer_queue.start_async({0: data}, output_layer_onnx)\n",
    "    infer_queue.wait_all()\n",
    "    end = time.time()\n",
    "    print('Inference took {:.2f} ms in average'.format((end-start)/iterations*1000))\n",
    "    print('Total time: {:.2f} s'.format(end-start)+\" for \"+str(iterations)+\" runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ce20b07-0bb8-463b-9059-210b5a0a243a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer command to convert the ONNX model to OpenVINO:\n",
      "mo --input_model \"resnet50.onnx\" --output_dir \".\"\n",
      "Exporting ONNX model to IR...\n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /home/raymondlo84/pytorch_ext/intel-extension-for-pytorch/examples/cpu/inference/resnet50.xml\n",
      "[ SUCCESS ] BIN file: /home/raymondlo84/pytorch_ext/intel-extension-for-pytorch/examples/cpu/inference/resnet50.bin\n"
     ]
    }
   ],
   "source": [
    "onnx_path = \"resnet50.onnx\"\n",
    "ir_path = \".\"\n",
    "ir_file = \"resnet50.xml\"\n",
    "\n",
    "torch.onnx.export(\n",
    "            model_native,\n",
    "            data,\n",
    "            onnx_path,\n",
    "        )\n",
    "\n",
    "# Construct the command for Model Optimizer.\n",
    "mo_command = f\"\"\"mo\n",
    "                 --input_model \"{onnx_path}\"\n",
    "                 --output_dir \"{ir_path}\"\n",
    "                 \"\"\"\n",
    "mo_command = \" \".join(mo_command.split())\n",
    "print(\"Model Optimizer command to convert the ONNX model to OpenVINO:\")\n",
    "print(mo_command)\n",
    "\n",
    "print(\"Exporting ONNX model to IR...\")\n",
    "mo_result = %sx $mo_command\n",
    "print(\"\\n\".join(mo_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "85d73b17-8b46-4c2f-aaba-458a8709bca0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: Intel(R) Xeon(R) Gold 6348 CPU @ 2.60GHz\n"
     ]
    }
   ],
   "source": [
    "# Load the network to OpenVINO Runtime.\n",
    "ie = Core()\n",
    "devices = ie.available_devices\n",
    "\n",
    "for device in devices:\n",
    "    device_name = ie.get_property(device, \"FULL_DEVICE_NAME\")\n",
    "    print(f\"{device}: {device_name}\")\n",
    "\n",
    "num_cores = os.cpu_count() \n",
    "\n",
    "model_onnx = ie.read_model(model=onnx_path)\n",
    "compiled_model_onnx = ie.compile_model(model=model_onnx, device_name=\"CPU\")\n",
    "#compiled_model_onnx = ie.compile_model(model=model_onnx, device_name=\"AUTO\", config={\"PERFORMANCE_HINT\":\"THROUGHPUT\"})\n",
    "\n",
    "output_layer_onnx = compiled_model_onnx.output(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65b607c1-fd9b-43a1-ac81-afe33a1fc722",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference took 7.44 ms in average\n",
      "Total time: 22.32 s for 3000 runs\n"
     ]
    }
   ],
   "source": [
    "# Run inference on the input image. (sync mode)\n",
    "inference_openvino(compiled_model_onnx, data, output_layer_onnx, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d0c641d0-9da6-4774-b273-abae7e36bc24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ConstOutput: names[495] shape[1,1000] type: f32>\n",
      "Inference took 2.96 ms in average\n",
      "Total time: 8.87 s for 3000 runs\n"
     ]
    }
   ],
   "source": [
    "# Run inference on the input image. (async mode)\n",
    "print(output_layer_onnx)\n",
    "inference_openvino_async(compiled_model_onnx, data, output_layer_onnx, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3da3a75-7027-4897-b5bb-e5b96f45230c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# async is needed to get best run performance out from all cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad392e02-fc6b-484b-8144-871bc8f09baf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[ WARNING ] Performance hint was not explicitly specified in command line. Device(CPU) performance hint will be set to LATENCY.\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 72.22 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input.1 (node: input.1) : f32 / [...] / [1,3,224,224]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     495 (node: 495) : f32 / [...] / [1,1000]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input.1 (node: input.1) : u8 / [N,C,H,W] / [1,3,224,224]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     495 (node: 495) : f32 / [...] / [1,1000]\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 277.70 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: torch_jit\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 2\n",
      "[ INFO ]   NUM_STREAMS: 2\n",
      "[ INFO ]   AFFINITY: Affinity.CORE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 56\n",
      "[ INFO ]   PERF_COUNT: False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT: PerformanceMode.LATENCY\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'input.1'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'input.1' with random values \n",
      "[Step 10/11] Measuring performance (Start inference synchronously, limits: 30000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 15.90 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "[ INFO ] Count:            2616 iterations\n",
      "[ INFO ] Duration:         30028.72 ms\n",
      "[ INFO ] Latency:\n",
      "[ INFO ]    Median:        4.83 ms\n",
      "[ INFO ]    Average:       11.41 ms\n",
      "[ INFO ]    Min:           4.26 ms\n",
      "[ INFO ]    Max:           43.80 ms\n",
      "[ INFO ] Throughput:   206.96 FPS\n"
     ]
    }
   ],
   "source": [
    "!benchmark_app -m resnet50.xml -t 30 -api sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f30be10-0316-4879-aa8c-7b724fe46f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see the median average and min for all inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f004bfa5-571c-45b0-b7f3-824187ba837a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[ WARNING ] Performance hint was not explicitly specified in command line. Device(CPU) performance hint will be set to THROUGHPUT.\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 83.09 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input.1 (node: input.1) : f32 / [...] / [1,3,224,224]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     495 (node: 495) : f32 / [...] / [1,1000]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input.1 (node: input.1) : u8 / [N,C,H,W] / [1,3,224,224]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     495 (node: 495) : f32 / [...] / [1,1000]\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 3041.86 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: torch_jit\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 56\n",
      "[ INFO ]   NUM_STREAMS: 56\n",
      "[ INFO ]   AFFINITY: Affinity.CORE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 56\n",
      "[ INFO ]   PERF_COUNT: False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT: PerformanceMode.THROUGHPUT\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'input.1'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'input.1' with random values \n",
      "[Step 10/11] Measuring performance (Start inference asynchronously, 56 inference requests, limits: 30000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 71.77 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "[ INFO ] Count:            23408 iterations\n",
      "[ INFO ] Duration:         30116.91 ms\n",
      "[ INFO ] Latency:\n",
      "[ INFO ]    Median:        71.38 ms\n",
      "[ INFO ]    Average:       71.70 ms\n",
      "[ INFO ]    Min:           61.25 ms\n",
      "[ INFO ]    Max:           165.94 ms\n",
      "[ INFO ] Throughput:   777.24 FPS\n"
     ]
    }
   ],
   "source": [
    "!benchmark_app -m resnet50.xml -t 30 -api async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8269ad-ee53-4465-a2e9-c330e6397229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see the median average and min for all inferences-- and async usually have a slightly higher latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d369d40-2a76-4aac-97ac-881226afec28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
